{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Original Keywords: oeuf gr\n",
      "Generated Query: oeuf or gr\n",
      "No results found!\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download(\"stopwords\",quiet=True)\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "class Query:\n",
    "    input_li = []\n",
    "    # op_li = []\n",
    "\n",
    "    def tokenize_seq(self,new_s):\n",
    "        new_s = new_s.lower()\n",
    "        translate_table = dict((ord(char), \" \") for char in string.punctuation)   \n",
    "        new_s = new_s.translate(translate_table)\n",
    "        li = word_tokenize(new_s)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filter_li = []\n",
    "        for words in li:\n",
    "            if(words not in stop_words):\n",
    "                filter_li.append(words)\n",
    "        return filter_li\n",
    "\n",
    "    def __init__(self,input_seq):\n",
    "        input_li = self.tokenize_seq(input_seq)\n",
    "        # op_li = []\n",
    "        # first_op_li = []\n",
    "        # if(op_seq != \"\"):\n",
    "        #     first_op_li = op_seq.split(\",\")\n",
    "        #     for i in range(len(first_op_li)):\n",
    "        #         temp_l = first_op_li[i].split()\n",
    "        #         for j in temp_l:\n",
    "        #             op_li.append(j)\n",
    "        # for i in range(len(op_li)):\n",
    "        #     op_li[i] = (op_li[i].strip()).lower()\n",
    "        self.input_li = input_li\n",
    "        # self.op_li = op_li\n",
    "    \n",
    "    def getQuery(self):\n",
    "        return self.input_li\n",
    "\n",
    "class Inverted_Index:\n",
    "\n",
    "    inverted_ind = {}\n",
    "    # universal_set = set()\n",
    "    # comparisons = 0\n",
    "    # document_count = 0\n",
    "    # name_arr = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.inverted_ind = {}\n",
    "        # self.universal_set = set()\n",
    "        # self.comparisons = 0\n",
    "        # self.document_count = 0\n",
    "        # self.name_arr = []\n",
    "       \n",
    "    # def extract_text(self,s,find_tags = False, remove_separators = False):\n",
    "    #     new_s = s[::]\n",
    "    #     if(find_tags):\n",
    "    #         start = [0,0] #Start is inclusive \n",
    "    #         end = [0,0] #End is exclusive\n",
    "    #         for i in range(len(s)):\n",
    "    #             if(i + 7 <= len(s) and s[i:i+7] == \"<TITLE>\"):\n",
    "    #                 start[0] = i+7\n",
    "    #             elif(i + 8 <= len(s) and s[i:i+8] == \"</TITLE>\"):\n",
    "    #                 end[0] = i\n",
    "    #             elif(i + 6 <= len(s) and s[i:i+6] == \"<TEXT>\"):\n",
    "    #                 start[1] = i+6\n",
    "    #             elif(i + 7 <= len(s) and s[i:i+7] == \"</TEXT>\"):\n",
    "    #                 end[1] = i\n",
    "    #         new_s = s[start[0]:end[0]] + \" \" + s[start[1]:end[1]]\n",
    "    #     if(remove_separators):\n",
    "    #         new_s = \" \".join(new_s.split(\"\\n\"))\n",
    "    #         new_s = \" \".join(new_s.split(\"-\"))\n",
    "    #     return new_s\n",
    "    \n",
    "    def addDoc(self,token_list,title): #token list in order, id of parent document \n",
    "        for i in range(len(token_list)):\n",
    "            key = token_list[i]\n",
    "            if(key in self.inverted_ind.keys()):\n",
    "                self.inverted_ind[str(key)].append(title)\n",
    "            else:\n",
    "                li = [title]\n",
    "                self.inverted_ind[str(key)] = li\n",
    "            # self.universal_set.add(title)\n",
    "                \n",
    "    def new_Data(self,title,text):\n",
    "        # f = open(path,\"r\")\n",
    "        # s = f.read()\n",
    "        # f.close()\n",
    "        # new_s = self.extract_text(s)\n",
    "        new_s = text\n",
    "        new_s = new_s.lower()\n",
    "        \n",
    "        translate_table = dict((ord(char), \" \") for char in string.punctuation)   \n",
    "        new_s = new_s.translate(translate_table)\n",
    "        li = word_tokenize(new_s)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filter_li = []\n",
    "        for words in li:\n",
    "            if(words not in stop_words):\n",
    "                filter_li.append(words)\n",
    "        filter_li = list(set(filter_li))\n",
    "        self.addDoc(filter_li,title)\n",
    "        # self.name_arr.append(path)\n",
    "        # self.document_count+=1\n",
    "\n",
    "    def showWord(self,key):\n",
    "        return self.inverted_ind[str(key)]\n",
    "    \n",
    "    def getFreq(self,key):\n",
    "        return len(self.inverted_ind[key])\n",
    "\n",
    "    # def simplify_not(self,op_seq):\n",
    "    #     if(op_seq == []):\n",
    "    #         return op_seq\n",
    "    #     simplified_seq = []\n",
    "    #     not_count = 1\n",
    "    #     for i in range(len(op_seq)-1):\n",
    "    #         if(op_seq[i+1] == op_seq[i] and op_seq[i] == \"not\"):\n",
    "    #             not_count+=1\n",
    "    #         else:\n",
    "    #             if(op_seq[i] == \"not\"):\n",
    "    #                 if(not_count%2 == 1):\n",
    "    #                     simplified_seq.append(op_seq[i])\n",
    "    #                 not_count = 1\n",
    "    #             else:\n",
    "    #                 simplified_seq.append(op_seq[i])\n",
    "    #     if(op_seq[len(op_seq)-1] == \"not\"):\n",
    "    #         if(not_count%2 == 1):\n",
    "    #             simplified_seq.append(op_seq[len(op_seq)-1])\n",
    "    #     else:\n",
    "    #         simplified_seq.append(op_seq[len(op_seq)-1])\n",
    "    #     return simplified_seq\n",
    "    \n",
    "    def processQuery(self,input_seq):\n",
    "        # self.comparisons = 0\n",
    "        query = Query(input_seq)\n",
    "        input_li = query.getQuery()\n",
    "        op_li = ['or' for i in range(len(input_li)-1)]\n",
    "        og_qry = self.getStringQuery(input_li,op_li)\n",
    "        # op_li = self.simplify_not(op_li)\n",
    "        # str_qry = self.getStringQuery(input_li,op_li)\n",
    "        input_li_new = []\n",
    "        for i in range(len(input_li)):\n",
    "            if(input_li[i] in self.inverted_ind.keys()):\n",
    "                input_li_new.append(self.inverted_ind[input_li[i]])\n",
    "            \n",
    "        output = self.query_sched(input_li_new,op_li)\n",
    "        # comp_ans = self.comparisons\n",
    "        # self.comparisons = 0\n",
    "        return output,og_qry\n",
    "    \n",
    "    def getStringQuery(self,input_li,op_li):\n",
    "        ans = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while(i < len(op_li) and j < len(input_li)):\n",
    "            if(op_li[i] == \"not\"):\n",
    "                ans.append(op_li[i])\n",
    "                i+=1\n",
    "            else:\n",
    "                ans.append(input_li[j])\n",
    "                j+=1\n",
    "                ans.append(op_li[i])\n",
    "                i+=1\n",
    "        if(j < len(input_li)):\n",
    "            ans.append(input_li[j])\n",
    "        final_ans = \" \".join(ans)\n",
    "        return final_ans\n",
    "\n",
    "    def getOutput(self,input_seq):\n",
    "        print(\"Original Keywords:\",inp_seq)\n",
    "        output, og_qry = self.processQuery(input_seq)\n",
    "        print(\"Generated Query:\",og_qry)\n",
    "        # print(\"Simplified Input Query:\",str_qry)\n",
    "        # print(output)\n",
    "        \n",
    "        \n",
    "        if(len(output) > 0 and len(output[0])>0):\n",
    "            # print(\"Number of Documents:\",len(output[0]))\n",
    "            print(\"Movie Names:\")\n",
    "            for j in range(len(output[0])):\n",
    "                print(str(j) + \". \" + output[0][j])\n",
    "                # print(\"\\t\",str(j+1)+\".\",self.name_arr[output[0][j]])\n",
    "        else:\n",
    "            print(\"No results found!\")\n",
    "        # print(\"Number of comparisons for fetching result:\",comp_ans)\n",
    "\n",
    "    \n",
    "    def query_sched(self,input_li,op_li): #input_li is list of lists, each list in input_li is the doc_list of a regex\n",
    "        if(len(op_li) == 0):\n",
    "            return input_li\n",
    "        # elif(op_li[0] == \"not\"):\n",
    "        #     output = self.query_not(input_li[0])\n",
    "        #     return self.query_sched([output]+input_li[1:],op_li[1:])\n",
    "        # elif(op_li[0] == \"and\"):\n",
    "        #     if(len(op_li) > 1 and op_li[1] == \"not\"):\n",
    "        #         op_li[0] = \"not\"\n",
    "        #         op_li[1] = \"and\"\n",
    "        #         temp = input_li[0]\n",
    "        #         input_li[0] = input_li[1]\n",
    "        #         input_li[1] = temp\n",
    "        #         return self.query_sched(input_li,op_li)\n",
    "        #     else:\n",
    "        #         output = self.query_and(input_li[0],input_li[1])\n",
    "        elif(op_li[0]  == \"or\"):\n",
    "            # if(len(op_li) > 1 and op_li[1] == \"not\"):\n",
    "            #     op_li[0] = \"not\"\n",
    "            #     op_li[1] = \"or\"\n",
    "            #     temp = input_li[0]\n",
    "            #     input_li[0] = input_li[1]\n",
    "            #     input_li[1] = temp\n",
    "            #     return self.query_sched(input_li,op_li)\n",
    "            # else:\n",
    "            # print(input_li)\n",
    "            if len(input_li)<2:\n",
    "                output = input_li\n",
    "            # elif len(input_li[0][1])==0:\n",
    "            #     output = input_li[0]\n",
    "            else:\n",
    "                output = self.query_or(input_li[0],input_li[1])\n",
    "\n",
    "        new_l = [output]+ input_li[2:]\n",
    "        return self.query_sched(new_l,op_li[1:])\n",
    "\n",
    "    # def query_and(self,t1,t2):\n",
    "    #     t1_li = t1\n",
    "    #     t2_li = t2\n",
    "    #     merge_li = []\n",
    "    #     st = 0\n",
    "    #     st2 = 0\n",
    "    #     while(st<len(t1_li) and st2 < len(t2_li)):\n",
    "    #         if(t1_li[st] == t2_li[st2]):\n",
    "    #             merge_li.append(t1_li[st])\n",
    "    #             st +=1\n",
    "    #             st2 +=1\n",
    "    #         elif(t1_li[st]<t2_li[st2]):\n",
    "    #             st +=1\n",
    "    #         else:\n",
    "    #             st2+=1\n",
    "    #         self.comparisons+=1\n",
    "    #     return merge_li\n",
    "\n",
    "    def query_or(self,a,b):\n",
    "        a_list = a\n",
    "        b_list = b\n",
    "        ait = 0\n",
    "        bit = 0\n",
    "        output = []\n",
    "        while(ait < len(a_list) and bit < len(b_list)):\n",
    "            if(a[ait] < b[bit]):\n",
    "                output.append(a[ait])\n",
    "                ait+=1\n",
    "            elif(a[ait] == b[bit]):\n",
    "                output.append(a[ait])\n",
    "                ait+=1\n",
    "                bit+=1\n",
    "            else:\n",
    "                output.append(b[bit])\n",
    "                bit+=1\n",
    "            # self.comparisons+=1\n",
    "        while(ait < len(a_list)):\n",
    "            output.append(a[ait])\n",
    "            ait+=1\n",
    "        while(bit < len(b_list)):\n",
    "            output.append(b[bit])\n",
    "            bit+=1\n",
    "        return output \n",
    "    \n",
    "    # def query_not(self,a):\n",
    "    #     univ = list(self.universal_set)\n",
    "    #     univ.sort()\n",
    "    #     output = []\n",
    "    #     self.comparisons = 0\n",
    "    #     ait = 0\n",
    "    #     for i in range(len(univ)):\n",
    "    #         if(ait < len(a)):\n",
    "    #             self.comparisons+=1\n",
    "    #         if(ait < len(a) and univ[i] == a[ait]):\n",
    "    #             ait+=1\n",
    "    #         else:\n",
    "    #             output.append(univ[i])\n",
    "    #     return output\n",
    "\n",
    "def getNum(i):\n",
    "    return (4-len(str(i)))*\"0\" + str(i)\n",
    "\n",
    "try:\n",
    "    inverted_index = pickle.load(open(\"baseline1_savefile.pickle\", \"rb\"))\n",
    "    # print(inverted_index.inverted_ind.keys())\n",
    "except (OSError, IOError) as e:\n",
    "    inverted_index = Inverted_Index()\n",
    "    df = pd.read_csv('merged_genre.csv')\n",
    "\n",
    "    for ind in df.index:\n",
    "        f = inverted_index.new_Data(str(df['title'][ind]), str(df['title'][ind]) + \" \" + str(df['cast'][ind]) + \" \" + str(df['listed_in'][ind]) + \" \" + str(df['description'][ind]))\n",
    "    # for j in range(1,1401): # 1,1401\n",
    "    #     f = inverted_index.new_Data(\"Data/CSE508_Winter2023_Dataset/cranfield\"+getNum(j))\n",
    "    pickle.dump(inverted_index, open(\"baseline1_savefile.pickle\", \"wb\"))\n",
    "\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "n = int(input())\n",
    "for i in range(n):\n",
    "    inp_seq = input()\n",
    "    # op_seq = input() \n",
    "    output = inverted_index.getOutput(inp_seq)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10a6afa7f546b8aeddabb531bd09b33e3634fe1e44cc86f33fc43c56ed89614d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
