{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{464: [5], 7089: [5], 7990: [3], 7991: [5]}\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Search Phrase: jurassic world\n",
      "Number of Documents Retrieved: 0\n",
      "No results found!\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download(\"stopwords\",quiet=True)\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "class Query:\n",
    "    input_li = []\n",
    "    # op_li = []\n",
    "\n",
    "    def tokenize_seq(self,new_s):\n",
    "        new_s = new_s.lower()\n",
    "        translate_table = dict((ord(char), \" \") for char in string.punctuation)   \n",
    "        new_s = new_s.translate(translate_table)\n",
    "        li = word_tokenize(new_s)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filter_li = []\n",
    "        for words in li:\n",
    "            if(words not in stop_words):\n",
    "                filter_li.append(words)\n",
    "        return filter_li\n",
    "\n",
    "    def __init__(self,input_seq):\n",
    "        input_li = self.tokenize_seq(input_seq)\n",
    "        # op_li = []\n",
    "        # first_op_li = []\n",
    "        # if(op_seq != \"\"):\n",
    "        #     first_op_li = op_seq.split(\",\")\n",
    "        #     for i in range(len(first_op_li)):\n",
    "        #         temp_l = first_op_li[i].split()\n",
    "        #         for j in temp_l:\n",
    "        #             op_li.append(j)\n",
    "        # for i in range(len(op_li)):\n",
    "        #     op_li[i] = (op_li[i].strip()).lower()\n",
    "        self.input_li = input_li\n",
    "        # self.op_li = op_li\n",
    "    \n",
    "    def getQuery(self):\n",
    "        return self.input_li\n",
    "\n",
    "class Positional_Inverted_Index:\n",
    "    inverted_ind = {} # {word:dict} #dict{id:list}\n",
    "    document_count = 0\n",
    "    name_arr = []\n",
    "    word_freq = {}\n",
    "\n",
    "    # def extract_text(self,s,find_tags = False, remove_separators = False):\n",
    "    #     new_s = s[::]\n",
    "    #     if(find_tags):\n",
    "    #         start = [0,0] #Start is inclusive \n",
    "    #         end = [0,0] #End is exclusive\n",
    "    #         for i in range(len(s)):\n",
    "    #             if(i + 7 <= len(s) and s[i:i+7] == \"<TITLE>\"):\n",
    "    #                 start[0] = i+7\n",
    "    #             elif(i + 8 <= len(s) and s[i:i+8] == \"</TITLE>\"):\n",
    "    #                 end[0] = i\n",
    "    #             elif(i + 6 <= len(s) and s[i:i+6] == \"<TEXT>\"):\n",
    "    #                 start[1] = i+6\n",
    "    #             elif(i + 7 <= len(s) and s[i:i+7] == \"</TEXT>\"):\n",
    "    #                 end[1] = i\n",
    "    #         new_s = s[start[0]:end[0]] + \" \" + s[start[1]:end[1]]\n",
    "    #     if(remove_separators):\n",
    "    #         new_s = \" \".join(new_s.split(\"\\n\"))\n",
    "    #         new_s = \" \".join(new_s.split(\"-\"))\n",
    "    #     return new_s\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.inverted_ind = {}\n",
    "        self.document_count = 0\n",
    "        self.name_arr = []\n",
    "        self.word_freq = {}\n",
    "\n",
    "    def new_Data(self,title,text):\n",
    "        # f = open(path,\"r\")\n",
    "        # s = f.read()\n",
    "        # f.close()\n",
    "        # new_s = self.extract_text(s)\n",
    "        new_s = text\n",
    "        new_s = new_s.lower()\n",
    "        \n",
    "        translate_table = dict((ord(char), \" \") for char in string.punctuation)   \n",
    "        new_s = new_s.translate(translate_table)\n",
    "        li = word_tokenize(new_s)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filter_li = []\n",
    "        for words in li:\n",
    "            if(words not in stop_words):\n",
    "                filter_li.append(words)\n",
    "        filter_li = list(set(filter_li))\n",
    "        self.addDoc(filter_li,title)\n",
    "        # self.name_arr.append(path)\n",
    "        # self.document_count+=1\n",
    "    \n",
    "    def addDoc(self,token_list,doc_id): #token list in order, id of parent document \n",
    "        for i in range(len(token_list)):\n",
    "            key = token_list[i]\n",
    "            if(key in self.inverted_ind.keys()):\n",
    "                self.word_freq[key]+=1\n",
    "                dict = self.inverted_ind[str(key)]\n",
    "                if(int(doc_id) not in dict.keys()):\n",
    "                    dict[int(doc_id)] = [i]\n",
    "                else:\n",
    "                    dict[int(doc_id)].append(i)\n",
    "            else:\n",
    "                self.word_freq[key] = 1\n",
    "                dict = {}\n",
    "                dict[int(doc_id)] = [i]\n",
    "                self.inverted_ind[str(key)] = dict\n",
    "\n",
    "    def getPositionalList(self,key):\n",
    "        return self.inverted_ind[str(key)]\n",
    "        \n",
    "    def binary_search(self,arr, x):\n",
    "        low = 0\n",
    "        high = len(arr) - 1\n",
    "        mid = 0\n",
    "        while low <= high:\n",
    "            mid = (high + low) // 2\n",
    "            if arr[mid] < x:\n",
    "                low = mid + 1\n",
    "            elif arr[mid] > x:\n",
    "                high = mid - 1\n",
    "            else:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def getFreq(self,key):\n",
    "        return len(self.inverted_ind[key])\n",
    "    \n",
    "    def check_existence(self,word,document_id,position):\n",
    "        if word in self.inverted_ind.keys():\n",
    "            if document_id in self.inverted_ind[word].keys():\n",
    "                return self.binary_search(self.inverted_ind[word][document_id],position)\n",
    "            else:\n",
    "                return -1\n",
    "        return -2\n",
    "\n",
    "    def processHelper(self,input_li):\n",
    "        output = []\n",
    "        override = 1\n",
    "        if input_li[0] not in self.inverted_ind.keys():\n",
    "            return output\n",
    "        for cur_doc in self.inverted_ind[input_li[0]].keys():\n",
    "            for cur_pos in self.inverted_ind[input_li[0]][cur_doc]:\n",
    "                next_pos = cur_pos+1\n",
    "                j = 1\n",
    "                while(j< len(input_li)):\n",
    "                    x = self.check_existence(input_li[j],cur_doc,next_pos)\n",
    "                    j+=1\n",
    "                    next_pos+=1\n",
    "                    override = x\n",
    "                    if(override <= 0):\n",
    "                        break\n",
    "                if(override <= -1):\n",
    "                    break\n",
    "                if(j == len(input_li)):\n",
    "                    output.append(cur_doc)\n",
    "            if(override <= -2):\n",
    "                break\n",
    "        return output\n",
    "    \n",
    "    def getOutput(self,input_seq,show_names = False):\n",
    "        query = Query(input_seq)\n",
    "        input_li= query.getQuery()\n",
    "        if(len(input_li) > 0):\n",
    "            input_li.append(input_li[-1])\n",
    "        output = list(set(self.processHelper(input_li)))\n",
    "        output.sort()\n",
    "        # if(show_names):\n",
    "        #     out_list = []\n",
    "        #     for i in output:\n",
    "        #         out_list.append(self.name_arr[i])\n",
    "        #     return out_list\n",
    "        return output\n",
    "\n",
    "def getNum(i):\n",
    "    return (4-len(str(i)))*\"0\" + str(i)\n",
    "\n",
    "\n",
    "try:\n",
    "    invertedIndex = pickle.load(open(\"baseline2_savefile.pickle\", \"rb\"))\n",
    "    print(invertedIndex.inverted_ind['jurassic'])\n",
    "    print(invertedIndex.inverted_ind['world'])\n",
    "except (OSError, IOError) as e:\n",
    "    invertedIndex = Positional_Inverted_Index()\n",
    "    df = pd.read_csv('merged_genre.csv')\n",
    "\n",
    "    for ind in df.index:\n",
    "        f = invertedIndex.new_Data(ind, str(df['title'][ind]) + \" \" + str(df['cast'][ind]) + \" \" + str(df['listed_in'][ind]) + \" \" + str(df['description'][ind]))\n",
    "    # for j in range(1,1401): # 1,1401\n",
    "    #     f = inverted_index.new_Data(\"Data/CSE508_Winter2023_Dataset/cranfield\"+getNum(j))\n",
    "    pickle.dump(invertedIndex, open(\"baseline2_savefile.pickle\", \"wb\"))\n",
    "\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "df = pd.read_csv('merged_genre.csv')\n",
    "n = int(input())\n",
    "\n",
    "for i in range(n):\n",
    "    inp_seq = input()\n",
    "    print(\"Search Phrase:\",inp_seq)\n",
    "    output = invertedIndex.getOutput(inp_seq,show_names = True)\n",
    "    print(\"Number of Documents Retrieved:\",len(output))\n",
    "    \n",
    "    if(len(output)>0):\n",
    "        print(\"Movie recommendations:\")\n",
    "        for j in range(len(output)):\n",
    "            print(\"\\t\",str(j+1)+\".\",df['title'][int(output[j])])\n",
    "    else:\n",
    "        print(\"No results found!\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10a6afa7f546b8aeddabb531bd09b33e3634fe1e44cc86f33fc43c56ed89614d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
